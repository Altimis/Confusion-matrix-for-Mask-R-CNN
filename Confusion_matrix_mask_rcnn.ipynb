{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the normalized vectors of gt and pred that will be needed in printing the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy these functions in your utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "\n",
    "Licence : AIT JEDDI Yassine\n",
    "\n",
    "Objectif : compute a confusion matrix for the whole test dataset and Mean Average Precision based on a IOU tresh (VOC)\n",
    "\n",
    "Reference : https://github.com/matterport/Mask_RCNN/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Note : copy the get_iou and gt_pred_lists in your utils.py file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#function 1 to be added to your utils.py\n",
    "def get_iou(a, b, epsilon=1e-5):\n",
    "    \"\"\" \n",
    "    Given two boxes `a` and `b` defined as a list of four numbers:\n",
    "            [x1,y1,x2,y2]\n",
    "        where:\n",
    "            x1,y1 represent the upper left corner\n",
    "            x2,y2 represent the lower right corner\n",
    "        It returns the Intersect of Union score for these two boxes.\n",
    "\n",
    "    Args: \n",
    "        a:          (list of 4 numbers) [x1,y1,x2,y2]\n",
    "        b:          (list of 4 numbers) [x1,y1,x2,y2]\n",
    "        epsilon:    (float) Small value to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "        (float) The Intersect of Union score.\n",
    "    \"\"\"\n",
    "    # COORDINATES OF THE INTERSECTION BOX\n",
    "    x1 = max(a[0], b[0])\n",
    "    y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2])\n",
    "    y2 = min(a[3], b[3])\n",
    "\n",
    "    # AREA OF OVERLAP - Area where the boxes intersect\n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "    # handle case where there is NO overlap\n",
    "    if (width<0) or (height <0):\n",
    "        return 0.0\n",
    "    area_overlap = width * height\n",
    "\n",
    "    # COMBINED AREA\n",
    "    area_a = (a[2] - a[0]) * (a[3] - a[1])\n",
    "    area_b = (b[2] - b[0]) * (b[3] - b[1])\n",
    "    area_combined = area_a + area_b - area_overlap\n",
    "\n",
    "    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n",
    "    iou = area_overlap / (area_combined+epsilon)\n",
    "    return iou\n",
    "\n",
    "\n",
    "#function 2 to be added to your utils.py\n",
    "def gt_pred_lists(gt_class_ids, gt_bboxes, pred_class_ids, pred_bboxes, iou_tresh = 0.5):\n",
    "\n",
    "    \"\"\" \n",
    "        Given a list of ground truth and predicted classes and their boxes, \n",
    "        this function associates the predicted classes to their gt classes using a given Iou (Iou>= 0.5 for example) and returns \n",
    "        two normalized lists of len = N containing the gt and predicted classes, \n",
    "        filling the non-predicted and miss-predicted classes by the background class (index 0).\n",
    "\n",
    "        Args    :\n",
    "            gt_class_ids   :    list of gt classes of size N1\n",
    "            pred_class_ids :    list of predicted classes of size N2\n",
    "            gt_bboxes      :    list of gt boxes [N1, (x1, y1, x2, y2)]\n",
    "            pred_bboxes    :    list of pred boxes [N2, (x1, y1, x2, y2)]\n",
    "            \n",
    "        Returns : \n",
    "            gt             :    list of size N\n",
    "            pred           :    list of size N \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #dict containing the state of each gt and predicted class (0 : not associated to any other class, 1 : associated to a classe)\n",
    "    gt_class_ids_ = {'state' : [0*i for i in range(len(gt_class_ids))], \"gt_class_ids\":list(gt_class_ids)}\n",
    "    pred_class_ids_ = {'state' : [0*i for i in range(len(pred_class_ids))], \"pred_class_ids\":list(pred_class_ids)}\n",
    "\n",
    "    #the two lists to be returned\n",
    "    pred=[]\n",
    "    gt=[]\n",
    "\n",
    "    for i, gt_class in enumerate(gt_class_ids_[\"gt_class_ids\"]):\n",
    "        for j, pred_class in enumerate(pred_class_ids_['pred_class_ids']): \n",
    "            #check if the gt object is overlapping with a predicted object\n",
    "            if get_iou(gt_bboxes[i], pred_bboxes[j])>=iou_tresh:\n",
    "                #change the state of the gt and predicted class when an overlapping is found\n",
    "                gt_class_ids_['state'][i] = 1\n",
    "                pred_class_ids_['state'][j] = 1\n",
    "                #chack if the overlapping objects are from the same class\n",
    "                if (gt_class == pred_class):\n",
    "                    gt.append(gt_class)\n",
    "                    pred.append(pred_class)\n",
    "                #if the overlapping objects are not from the same class \n",
    "                else : \n",
    "                    gt.append(gt_class)\n",
    "                    pred.append(pred_class)\n",
    "    #look for objects that are not predicted (gt objects that dont exists in pred objects)\n",
    "    for i, gt_class in enumerate(gt_class_ids_[\"gt_class_ids\"]):\n",
    "        if gt_class_ids_['state'][i] == 0:\n",
    "            gt.append(gt_class)\n",
    "            pred.append(0)\n",
    "            #match_id += 1\n",
    "    #look for objects that are mispredicted (pred objects that dont exists in gt objects)\n",
    "    for j, pred_class in enumerate(pred_class_ids_[\"pred_class_ids\"]):\n",
    "        if pred_class_ids_['state'][j] == 0:\n",
    "            gt.append(0)\n",
    "            pred.append(pred_class)\n",
    "    return gt, pred\n",
    "\n",
    "\n",
    "\n",
    "#compute average precision in VOC way\n",
    "def voc_ap(tp, fp, fn):\n",
    "    \"\"\"\n",
    "    tp = true positif vector (each element represent the true positifs of each class)\n",
    "    fp = false positif\n",
    "    fn = false negatif\n",
    "    return : average precision, mean recall and mean precision\n",
    "    \"\"\"\n",
    "    \n",
    "    #recall and precision : \n",
    "    class_num = len(tp)-1 #eliminate the BG class\n",
    "    \n",
    "    rec=[0]*class_num\n",
    "    prec=[0]*class_num\n",
    "    for i in range(class_num):\n",
    "        rec[i] = tp[i]/(tp[i]+fn[i])\n",
    "        prec[i] =  tp[i]/(tp[i]+fp[i])\n",
    "        \n",
    "    rec=rec[1:] #eliminate the BG class\n",
    "    prec=prec[1:] #eliminate the BG class\n",
    "    \n",
    "    rec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    rec.append(1.0) # insert 1.0 at end of list\n",
    "    mrec = rec[:]\n",
    "    prec.insert(0, 0.0) # insert 0.0 at begining of list\n",
    "    prec.append(0.0) # insert 0.0 at end of list\n",
    "    mpre = prec[:]\n",
    "    \"\"\"\n",
    "     This part makes the precision monotonically decreasing\n",
    "        (goes from the end to the beginning)\n",
    "        matlab: for i=numel(mpre)-1:-1:1\n",
    "                    mpre(i)=max(mpre(i),mpre(i+1));\n",
    "    \"\"\"\n",
    "    # matlab indexes start in 1 but python in 0, so I have to do:\n",
    "    #     range(start=(len(mpre) - 2), end=0, step=-1)\n",
    "    # also the python function range excludes the end, resulting in:\n",
    "    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n",
    "    for i in range(len(mpre)-2, -1, -1):\n",
    "        mpre[i] = max(mpre[i], mpre[i+1])\n",
    "    \"\"\"\n",
    "     This part creates a list of indexes where the recall changes\n",
    "        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n",
    "    \"\"\"\n",
    "    i_list = []\n",
    "    for i in range(1, len(mrec)):\n",
    "        if mrec[i] != mrec[i-1]:\n",
    "            i_list.append(i) # if it was matlab would be i + 1\n",
    "    \"\"\"\n",
    "     The Average Precision (AP) is the area under the curve\n",
    "        (numerical integration)\n",
    "        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n",
    "    \"\"\"\n",
    "    ap = 0.0\n",
    "    for i in i_list:\n",
    "        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n",
    "    return ap, mrec, mpre\n",
    "\n",
    "#plot the precision recall curv for the whole dataset\n",
    "def plot_precision_recall(AP, precisions, recalls):\n",
    "    \"\"\"Draw the precision-recall curve.\n",
    "\n",
    "    AP: Average precision at IoU >= 0.5\n",
    "    precisions: list of precision values\n",
    "    recalls: list of recall values\n",
    "    \"\"\"\n",
    "    # Plot the Precision-Recall curve\n",
    "    _, ax = plt.subplots(1)\n",
    "    ax.set_title(\"Precision-Recall Curve. AP@50 = {:.3f}\".format(AP))\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    _ = ax.plot(recalls, precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print confusion matrix for the whole dataset and return tp,fp and fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy these functions in your utils.py also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.collections import QuadMesh\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "def get_new_fig(fn, figsize=[9,9]):\n",
    "    \"\"\" Init graphics \"\"\"\n",
    "    fig1 = plt.figure(fn, figsize)\n",
    "    ax1 = fig1.gca()   #Get Current Axis\n",
    "    ax1.cla() # clear existing plot\n",
    "    return fig1, ax1\n",
    "#\n",
    "\n",
    "def configcell_text_and_colors(array_df, lin, col, oText, facecolors, posi, fz, fmt, show_null_values=0):\n",
    "    \"\"\"\n",
    "      config cell text and colors\n",
    "      and return text elements to add and to dell\n",
    "      @TODO: use fmt\n",
    "    \"\"\"\n",
    "    text_add = []; text_del = [];\n",
    "    cell_val = array_df[lin][col]\n",
    "    tot_all = array_df[-1][-1]\n",
    "    per = (float(cell_val) / tot_all) * 100\n",
    "    curr_column = array_df[:,col]\n",
    "    ccl = len(curr_column)\n",
    "\n",
    "    #last line  and/or last column\n",
    "    if(col == (ccl - 1)) or (lin == (ccl - 1)):\n",
    "        #tots and percents\n",
    "        if(cell_val != 0):\n",
    "            if(col == ccl - 1) and (lin == ccl - 1):\n",
    "                tot_rig = 0\n",
    "                for i in range(array_df.shape[0] - 1):\n",
    "                    tot_rig += array_df[i][i]\n",
    "                per_ok = (float(tot_rig) / cell_val) * 100\n",
    "            elif(col == ccl - 1):\n",
    "                tot_rig = array_df[lin][lin]\n",
    "                per_ok = (float(tot_rig) / cell_val) * 100\n",
    "            elif(lin == ccl - 1):\n",
    "                tot_rig = array_df[col][col]\n",
    "                per_ok = (float(tot_rig) / cell_val) * 100\n",
    "            per_err = 100 - per_ok\n",
    "        else:\n",
    "            per_ok = per_err = 0\n",
    "\n",
    "        per_ok_s = ['%.2f%%'%(per_ok), '100%'] [per_ok == 100]\n",
    "\n",
    "        #text to DEL\n",
    "        text_del.append(oText)\n",
    "\n",
    "        #text to ADD\n",
    "        font_prop = fm.FontProperties(weight='bold', size=fz)\n",
    "        text_kwargs = dict(color='w', ha=\"center\", va=\"center\", gid='sum', fontproperties=font_prop)\n",
    "        lis_txt = ['%d'%(cell_val), per_ok_s, '%.2f%%'%(per_err)]\n",
    "        lis_kwa = [text_kwargs]\n",
    "        dic = text_kwargs.copy(); dic['color'] = 'g'; lis_kwa.append(dic);\n",
    "        dic = text_kwargs.copy(); dic['color'] = 'r'; lis_kwa.append(dic);\n",
    "        lis_pos = [(oText._x, oText._y-0.3), (oText._x, oText._y), (oText._x, oText._y+0.3)]\n",
    "        for i in range(len(lis_txt)):\n",
    "            newText = dict(x=lis_pos[i][0], y=lis_pos[i][1], text=lis_txt[i], kw=lis_kwa[i])\n",
    "            #print 'lin: %s, col: %s, newText: %s' %(lin, col, newText)\n",
    "            text_add.append(newText)\n",
    "        #print '\\n'\n",
    "\n",
    "        #set background color for sum cells (last line and last column)\n",
    "        carr = [0.27, 0.30, 0.27, 1.0]\n",
    "        if(col == ccl - 1) and (lin == ccl - 1):\n",
    "            carr = [0.17, 0.20, 0.17, 1.0]\n",
    "        facecolors[posi] = carr\n",
    "\n",
    "    else:\n",
    "        if(per > 0):\n",
    "            txt = '%s\\n%.2f%%' %(cell_val, per)\n",
    "        else:\n",
    "            if(show_null_values == 0):\n",
    "                txt = ''\n",
    "            elif(show_null_values == 1):\n",
    "                txt = '0'\n",
    "            else:\n",
    "                txt = '0\\n0.0%'\n",
    "        oText.set_text(txt)\n",
    "\n",
    "        #main diagonal\n",
    "        if(col == lin):\n",
    "            #set color of the textin the diagonal to white\n",
    "            oText.set_color('w')\n",
    "            # set background color in the diagonal to blue\n",
    "            facecolors[posi] = [0.35, 0.8, 0.55, 1.0]\n",
    "        else:\n",
    "            oText.set_color('r')\n",
    "\n",
    "    return text_add, text_del\n",
    "#\n",
    "\n",
    "def insert_totals(df_cm):\n",
    "    \"\"\" insert total column and line (the last ones) \"\"\"\n",
    "    sum_col = []\n",
    "    for c in df_cm.columns:\n",
    "        sum_col.append( df_cm[c].sum() )\n",
    "    sum_lin = []\n",
    "    for item_line in df_cm.iterrows():\n",
    "        sum_lin.append( item_line[1].sum() )\n",
    "    df_cm['sum_lin'] = sum_lin\n",
    "    sum_col.append(np.sum(sum_lin))\n",
    "    df_cm.loc['sum_col'] = sum_col\n",
    "    #print ('\\ndf_cm:\\n', df_cm, '\\n\\b\\n')\n",
    "#\n",
    "\n",
    "def pretty_plot_confusion_matrix(df_cm, annot=True, cmap=\"Oranges\", fmt='.2f', fz=11,\n",
    "      lw=0.5, cbar=False, figsize=[8,8], show_null_values=0, pred_val_axis='y'):\n",
    "    \"\"\"\n",
    "      print conf matrix with default layout (like matlab)\n",
    "      params:\n",
    "        df_cm          dataframe (pandas) without totals\n",
    "        annot          print text in each cell\n",
    "        cmap           Oranges,Oranges_r,YlGnBu,Blues,RdBu, ... see:\n",
    "        fz             fontsize\n",
    "        lw             linewidth\n",
    "        pred_val_axis  where to show the prediction values (x or y axis)\n",
    "                        'col' or 'x': show predicted values in columns (x axis) instead lines\n",
    "                        'lin' or 'y': show predicted values in lines   (y axis)\n",
    "    \"\"\"\n",
    "    if(pred_val_axis in ('col', 'x')):\n",
    "        xlbl = 'Predicted'\n",
    "        ylbl = 'Actual'\n",
    "    else:\n",
    "        xlbl = 'Actual'\n",
    "        ylbl = 'Predicted'\n",
    "        df_cm = df_cm.T\n",
    "\n",
    "    # create \"Total\" column\n",
    "    insert_totals(df_cm)\n",
    "\n",
    "    #this is for print allways in the same window\n",
    "    fig, ax1 = get_new_fig('Conf matrix default', figsize)\n",
    "\n",
    "    #thanks for seaborn\n",
    "    sn.set(font_scale=1.8)\n",
    "    ax = sn.heatmap(df_cm, annot=annot, annot_kws={\"size\": fz}, linewidths=lw, ax=ax1,\n",
    "                    cbar=cbar, cmap=cmap, linecolor='w', fmt=fmt)\n",
    "    \n",
    "\n",
    "    #set ticklabels rotation\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation = 75, fontsize = 26)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation = 25, fontsize = 26)\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    #face colors list\n",
    "    quadmesh = ax.findobj(QuadMesh)[0]\n",
    "    facecolors = quadmesh.get_facecolors()\n",
    "\n",
    "    #iter in text elements\n",
    "    array_df = np.array( df_cm.to_records(index=False).tolist() )\n",
    "    text_add = []; text_del = [];\n",
    "    posi = -1 #from left to right, bottom to top.\n",
    "    for t in ax.collections[0].axes.texts: #ax.texts:\n",
    "        pos = np.array( t.get_position()) - [0.5,0.5]\n",
    "        lin = int(pos[1]); col = int(pos[0]);\n",
    "        posi += 1\n",
    "        #print ('>>> pos: %s, posi: %s, val: %s, txt: %s' %(pos, posi, array_df[lin][col], t.get_text()))\n",
    "\n",
    "        #set text\n",
    "        txt_res = configcell_text_and_colors(array_df, lin, col, t, facecolors, posi, fz, fmt, show_null_values)\n",
    "\n",
    "        text_add.extend(txt_res[0])\n",
    "        text_del.extend(txt_res[1])\n",
    "\n",
    "    #remove the old ones\n",
    "    for item in text_del:\n",
    "        item.remove()\n",
    "    #append the new ones\n",
    "    for item in text_add:\n",
    "        ax.text(item['x'], item['y'], item['text'], **item['kw'])\n",
    "\n",
    "    #titles and legends\n",
    "    ax.set_title('Confusion matrix')\n",
    "    ax.set_xlabel(xlbl)\n",
    "    ax.set_ylabel(ylbl)\n",
    "    plt.tight_layout()  #set layout slim\n",
    "    plt.show()\n",
    "#\n",
    "\n",
    "def plot_confusion_matrix_from_data(y_test, predictions, columns=None, annot=True, cmap=\"Oranges\",\n",
    "      fmt='.2f', fz=11, lw=0.5, cbar=False, figsize=[24,24], show_null_values=0, pred_val_axis='lin'):\n",
    "    \"\"\"\n",
    "        plot confusion matrix function with y_test (actual values) and predictions (predic),\n",
    "        whitout a confusion matrix yet\n",
    "        return the tp, fp and fn to calculate the average precision \n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    #data\n",
    "    if(not columns):\n",
    "        #labels axis integer:\n",
    "        columns = range(1, len(np.unique(y_test))+1)\n",
    "        #labels axis string:\n",
    "        #from string import ascii_uppercase\n",
    "        #columns = ['class %s' %(i) for i in list(ascii_uppercase)[0:len(np.unique(y_test))]]\n",
    "    \n",
    "    confm = confusion_matrix(y_test, predictions)\n",
    "    num_classes = len(np.unique(y_test))\n",
    "    \n",
    "    #compute tp fn fp \n",
    "    \n",
    "    fp=[0]*num_classes\n",
    "    fn=[0]*num_classes\n",
    "    tp=[0]*num_classes\n",
    "    for i in range(confm.shape[0]):\n",
    "        fp[i]+=np.sum(confm[i])-np.diag(confm)[i]\n",
    "        fn[i]+=np.sum(np.transpose(confm)[i])-np.diag(confm)[i]\n",
    "        for j in range(confm.shape[1]):\n",
    "            if i==j:\n",
    "                tp[i]+=confm[i][j]\n",
    "\n",
    "    \n",
    "    #cmap = 'Oranges';\n",
    "    #fz = 24;\n",
    "    #figsize=[24,24];\n",
    "    #show_null_values = 2\n",
    "    df_cm = DataFrame(confm, index=columns, columns=columns)\n",
    "    '''\n",
    "    true_pos = np.diag(confm) \n",
    "    precision = np.sum(true_pos / np.sum(confm, axis=0))\n",
    "    recall = np.sum(true_pos / np.sum(confm, axis=1))\n",
    "    '''\n",
    "    \"\"\"\n",
    "    tp_and_fn = confm.sum(1)\n",
    "    tp_and_fp = confm.sum(0)\n",
    "    tp = confm.diagonal()\n",
    "\n",
    "    precision = tp / tp_and_fp\n",
    "    recall = tp / tp_and_fn\n",
    "    \"\"\"\n",
    "    pretty_plot_confusion_matrix(df_cm, fz=fz, cmap=cmap, figsize=figsize, show_null_values=show_null_values, pred_val_axis=pred_val_axis)\n",
    "    \n",
    "    return tp, fp, fn\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "#TEST functions\n",
    "#\n",
    "def _test_cm():\n",
    "    #test function with confusion matrix done\n",
    "    array = np.array( [[13,  0,  1,  0,  2,  0],\n",
    "                       [ 0, 50,  2,  0, 10,  0],\n",
    "                       [ 0, 13, 16,  0,  0,  3],\n",
    "                       [ 0,  0,  0, 13,  1,  0],\n",
    "                       [ 0, 40,  0,  1, 15,  0],\n",
    "                       [ 0,  0,  0,  0,  0, 20]])\n",
    "    #get pandas dataframe\n",
    "    df_cm = DataFrame(array, index=range(1,7), columns=range(1,7))\n",
    "    #colormap: see this and choose your more dear\n",
    "    cmap = 'PuRd'\n",
    "    pretty_plot_confusion_matrix(df_cm, cmap=cmap)\n",
    "#\n",
    "\n",
    "def _test_data_class(columns, y_test=[4,5,6,5,4,4,7,8], predic=[4,5,6,5,4,4,7,8]):\n",
    "    \"\"\" test function with y_test (actual values) and predictions (predic) \"\"\"\n",
    "    #data\n",
    "    y_test = np.array(y_test)\n",
    "    predic = np.array(predic)\n",
    "    \"\"\"\n",
    "      Examples to validate output (confusion matrix plot)\n",
    "        actual: 5 and prediction 1   >>  3\n",
    "        actual: 2 and prediction 4   >>  1\n",
    "        actual: 3 and prediction 4   >>  10\n",
    "    \"\"\"\n",
    "    annot = True;\n",
    "    cmap = 'Oranges';\n",
    "    fmt = '.2f'\n",
    "    lw = 0.6\n",
    "    cbar = False\n",
    "    show_null_values = 2\n",
    "    pred_val_axis = 'y'\n",
    "    #size::\n",
    "    fz = 24;\n",
    "    figsize = [36,36];\n",
    "    if(len(y_test) > 10):\n",
    "        fz=24; figsize=[36,36];\n",
    "    tp, fp, fn = plot_confusion_matrix_from_data(y_test, predic, columns,\n",
    "      annot, cmap, fmt, fz, lw, cbar, figsize, show_null_values, pred_val_axis)\n",
    "    \n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things you need to add to your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After you load your model and your test dataset, you are ready to do the predictions on the whole dataset. Go throgh all images in the dataset and save the gt and predictions in two vectors of the same size (gt_tot and pred_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After you save these two vectors, you will be ready to print your confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tp, fp and fn vectors returned are vectors that contains the tp fp and fn of each class.  You will used them to calculate the Mean Average Precision of each class in the same way used in papers (through the whole dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "gt_tot = np.array([])\n",
    "pred_tot = np.array([])\n",
    "mAP_ = []\n",
    "\n",
    "for image_id in dataset.image_ids:\n",
    "#image_id = random.choice(dataset.image_ids)\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n",
    "    info = dataset.image_info[image_id]\n",
    "    #print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "                                           #dataset.image_reference(image_id)))\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=1)\n",
    "\n",
    "    # Display results\n",
    "    #ax = get_ax(1)\n",
    "    r = results[0]\n",
    "    #visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                #dataset.class_names, r['scores'], ax=ax,\n",
    "                                #title=\"Predictions\")\n",
    "    #log(\"gt_class_id\", gt_class_id)\n",
    "    #log(\"gt_bbox\", gt_bbox)\n",
    "    #log(\"gt_mask\", gt_mask)\n",
    "    gt, pred = utils.gt_pred_lists(gt_class_id, gt_bbox, r['class_ids'], r['rois'])\n",
    "    gt_tot = np.append(gt_tot, gt)\n",
    "    pred_tot = np.append(pred_tot, pred)\n",
    "    #precision_, recall_, AP_ = utils.compute_precision_recall_map(gt_tot, pred_tot)\n",
    "    AP_, precision_, recall_, overlap_ = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                          r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "    print(\"the actual len of the gt vect is : \", len(gt_tot))\n",
    "    print(\"the actual len of the pred vect is : \", len(pred_tot))\n",
    "    mAP_.append(AP_)\n",
    "    print(\"Average precision of this image\")\n",
    "    print(\"The actual mean average precision for the whole images (matterport methode) \", sum(mAP_)/len(mAP_))\n",
    "    \n",
    "gt_tot=gt_tot.astype(int)\n",
    "pred_tot=pred_tot.astype(int)\n",
    "#save the vectors of gt and pred\n",
    "save_dir = \"ourput\"\n",
    "gt_pred_tot_json = {\"gt_tot\" : gt_tot, \"pred_tot\" : pred_tot}\n",
    "df = pd.DataFrame(gt_pred_tot_json)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedir(save_dir)\n",
    "    \n",
    "df.to_json(os.path.join(save_dir,\"gt_pred_test.json\"))\n",
    "\n",
    "#print the confusion matrix :\n",
    "\n",
    "tp, fp, fn = utils._test_data_class(dataset.class_names, gt_tot, pred_tot)\n",
    "\n",
    "ap,mrec,mprec=utils.voc_ap(tp, fp, fn)\n",
    "\n",
    "print(ap)\n",
    "\n",
    "#plot precision recall curve : \n",
    "\n",
    "utils.plot_precision_recall(ap, mprec, mrec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
